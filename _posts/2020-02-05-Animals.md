---
title: "Saving animals with Machine Learning"
date: 2020-11-17
tags: [data preprocessing, data cleaning, classification, gradient boosting, random forest, kaggle]
header:
 image: 
excerpt: "In this article, I build predictive models to predict what's going to happen to animals abandonned in shelters."
mathjax: "true"
---

Meet Gus and Skooter:

![alt]({{ site.url }}{{ site.baseurl }}/figures/guskooter.png)

Gus is a playful domestic shorthair cat. Skooter is a sweet tiny white poodle. The sad thing they share in common is that they've both been in an animal shelter (Gus for a year, and Skooter for two). How could we predict whether Gus and Skooter will ever get adopted ? 

About 100 000 cats and dogs are abandonned by their owners every year in France. Some of them are left to a cruel fate, and on some rare occasions have to be taken away from their owners for abuse.
The SPA (Société de Protection des Animaux) fosters about 40 000 of these animals. It also seems that summer is particularly bad season for the pets, according to their statistics. But what happens to all these animals ?
 
Unfortunately, with the ever-increasing number of abandonned animals, some can stay fostered long enough to die in the shelter. Others may even have to be euthanized. And a lucky few are offered a new home after being adopted.

![alt]({{ site.url }}{{ site.baseurl }}/figures/animals2.PNG)

Is it possible to predict what happens to fostered animals down the road using their characteristics (such as their age, breed, color, how long they have been in the shelter, etc) ? What's most likely going to happen to Gus and Skooter down the road ?

Using a dataset from the [Shelter Animal Outcomes (Kaggle Competition)](https://www.kaggle.com/c/shelter-animal-outcomes/overview), I'm going to build predictive models to predict the animals' destinies. Though the dataset comes from a Kaggle competition, I'm not focusing here on building the most accurate model possible. Instead, I will take my time preprocessing the data and cleaning it: as we're going to see, the real world data isn't always very tidy and requires some extra steps. 


![alt]({{ site.url }}{{ site.baseurl }}/figures/flow.PNG)

And also, just a foreword: I'll use the multiclass logarithmic function which is a way to measure how good the predictions are. After training the algorithm, I will determine for each animal $$i$$ all the probabilities for it to be in the class $$j$$, $$j$$ being one of those categories ('Adoption', 'Died', 'Euthanasia', 'Return to owner', and 'Transfer'). Then I will compute the following quantity:

$$LogLoss = \frac{-1}{N} \sum_{i = 1}^{N} \sum_{j = 1}^M y_{ij} log(p_{ij})$$

Where:

* $$M = 5$$ (number of outcome classes), 
* $$y_{ij} = 1$$ (if observation $$i$$ is in outcome $$j$$), $$y_{ij} = 0$$ (if observation $$i$$ is not in outcome $$j$$).


Let's get coding. I start by importing the relevant functions and to import and visualize the data:

```python
import itertools
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import GridSearchCV


complete_data = pd.read_csv('train.csv.gz')
complete_data.head()
```

![alt]({{ site.url }}{{ site.baseurl }}/figures/algo1.PNG)


## Dropping some features, converting some others
I'm going to drop the animal's names to begin with, as well as the animal ID's.
The 'OutcomeSubtype' is not necessary because it does not provide any additional information: it's only known after the 'Outcometype' has been determined (it is only a qualitative precision after the animal departs and including it might create a bias in the characteristics). 

I'll also make the SexUponOutcome 'atomic' (since it contains two information at the same time: whether the animal has been castrated and whether itis male or female). I'll split it into two columns. 

AgeUponOutcome represents the age in time units (years, months, weeks, days) but for the sake of simplicity, I'll adopt a year-base convention. Let's just say that $$1 \text{ year} \approx 365 \text{ days} \approx 52 \text{ weeks}$$.

All the variables (except for the age) are categories I'll later convert them to categorical variables.



```python
#Remove AnimalID, Name and Outcome Subtype
#Transform the 'Castrated' column into 0 for all Intact animals and 1 for all the Neutered and Spayed animals. 

complete_data = complete_data[complete_data.columns.difference(('AnimalID', 'Name', 'OutcomeSubtype'))].copy()

complete_data.loc[complete_data['Castrated']=='Intact', 'Castrated'] = 0
complete_data.loc[complete_data['Castrated'].isin(('Neutered', 'Spayed')), 'Castrated'] = 1

```
This is what the data looks like by now:

![alt]({{ site.url }}{{ site.baseurl }}/figures/algo2.PNG)



## Dealing with missing values

Now I'll deal with missing values, whether the label is 'unknown' or the cell is null. How many are there?
I noticed that less than 4% of the data had missing attributes in 'Castrated', so I just proceeded to delete all those entries. Same for the 'Sex' column.
After that is done, we no longer need the 'Sex upon outcome' column, since all its attributes were extracted. 

```python
(complete_data['Castrated'] == 'Unknown').sum() / complete_data.shape[0]
complete_data['Castrated'].isnull().sum() / complete_data.shape[0]
complete_data = complete_data.loc[complete_data['Castrated'].isin((0, 1))]

complete_data.loc[complete_data['Sex']=='Female', 'Sex'] = 0
complete_data.loc[complete_data['Sex']=='Male', 'Sex'] = 1
complete_data = complete_data.loc[complete_data['Sex'].isin((0, 1))]

complete_data = complete_data.loc[:, complete_data.columns != 'SexuponOutcome']

```

![alt]({{ site.url }}{{ site.baseurl }}/figures/algo3.PNG)

### Dealing with the Seasons

As I mentionned earlier, it seems that the period of the year changes something in the adoption rates. So an educated guess would be to take this into consideration while dealing with the "DateTime".
I'll map each of the pandas timestamps to itself minus the first day of the year and convert this time difference into a float using the 365-day convention: this is how we construct 'FractionofYear'. In other words, I'll convert the information in the "DateTime" to "fraction of the year" to retain the information about the period of the year we're in.

'FractionofDay' is exactly the same thing but for days (and a 24 hour/day convention). Then, these two columns are discarded.

```python
complete_data['DateTime'] = pd.to_datetime(complete_data['DateTime'], format='%Y-%m-%d %H:%M:%S')
complete_data['FractionOfYearFromBeginningOfYear'] = complete_data['DateTime'].map(
    lambda x: (x-pd.Timestamp(year=x.year, month=1, day=1)).days/365
)
complete_data['FractionOfDay'] = complete_data['DateTime'].map(lambda x: x.hour/24)

complete_data = complete_data.loc[:, complete_data.columns != 'DateTime']

```


![alt]({{ site.url }}{{ site.baseurl }}/figures/algo4.PNG)

### Converting the Animal Type Column's labels: 

A quick check of the 'AnimalType' column allows us to determine that we're classifying cats and dogs. 
I then convert the True/False boolean mask into "0"'s and "1"'s.

```python

complete_data['AnimalType'].unique()

complete_data['AnimalType'] = (complete_data['AnimalType']=='Dog').astype(np.int)

```

### Cleaning the messy 'AgeuponOutcome' column:

In the Ageuponoutcome column there are numerical values and different units: years, weeks. Furthermore sometimes for the same unit we have a singular or a plural. I'm going to implement a "period string to years" function as follows:
* First, split the column at the space
* Take the resulting integer n value as well as the associated string (the unit)
* Add an -s to this string  (make all units in plural)
* Convert the associated n to years (by distinguishing cases where string is years, months, weeks or days) using the above conventions






```python

def period_str_to_years(s):
    if pd.isnull(s):
        return None
    n, unit = s.split(' ')
    n = int(n)
    if unit[-1] != 's':
        unit = unit + 's'
    if unit == 'years':
        return n
    if unit == 'months':
        return n/12
    if unit == 'weeks':
        return n/52
    if unit == 'days':
        return n/365
    return None

#Apply the function str_to_years to our AgeuponOutcome column and delete it subsequently)
complete_data['AgeInYears'] = complete_data['AgeuponOutcome'].map(period_str_to_years)
complete_data = complete_data.loc[:, complete_data.columns != 'AgeuponOutcome']
complete_data.dropna(subset=('AgeInYears',), inplace=True)

```

We can now see each animal's age in years:

![alt]({{ site.url }}{{ site.baseurl }}/figures/algo5.PNG)




### Animal 'Breed' and 'Color' columns

Let us now deal with the Breed and Color columns. For the Breed, we shall look at the presence of the word 'Mix' as well as the presence of the separator ("/"). This indicates a mixed breed animal.
For the color, some animals are labelled as 'tabby' and other animals have multiple colors separated by a '/'. So I shall split Breed and Color according to these two cases. 
Then, I assign a binary variable for each column: 'The animal is tabby' or  'the animal is not tabby'.
The animal has 'mixed colors' or the animal has 'no mixed colors'.

```python
complete_data['Mixedbreed'] = (complete_data['Breed'].str.contains(' Mix', case=False) 
                               | complete_data['Breed'].str.contains('/')).astype(np.int)
complete_data = complete_data.loc[:, complete_data.columns != 'Breed']

complete_data['Tabby'] = complete_data['Color'].str.contains('Tabby', case=False).astype(np.int)
complete_data['MixedColors'] = complete_data['Color'].str.contains('/').astype(np.int)
complete_data = complete_data.loc[:, complete_data.columns != 'Color']
```

This is where we're at, by this point: 
![alt]({{ site.url }}{{ site.baseurl }}/figures/algo6.PNG)


### The final step of preprocessing: Converting the Outcomes' column into categorical variables

Since clearly $$M=5$$ (we have 5 possible outcomes for each animal), we can simply convert these labels using the following dictionary for convenience.:

```python
outcome_type_dict = {
    'Return_to_owner': 0,
    'Euthanasia': 1,
    'Adoption': 2,
    'Transfer': 3, 
    'Died': 4
}
outcome_type_dict_inv = {v: k for k, v in outcome_type_dict.items()}

complete_data['OutcomeType'] = complete_data['OutcomeType'].map(lambda x: outcome_type_dict[x])
```


![alt]({{ site.url }}{{ site.baseurl }}/figures/algo7.PNG)

Finally, that's much cleaner ! We can now proceed to building the predictive models.

## Splitting the training and testing sets (the old school way)

Of course, we could use the train_test_split function from scikit-learn ! But we could also do it old-school:

```python
np.random.seed(0)
train_idx = np.random.choice(complete_data.index, size=3*complete_data.index.size//4, replace=False)
test_idx = complete_data.index.difference(train_idx)
train = complete_data.loc[train_idx]
test = complete_data.loc[test_idx]

X_train, Y_train = train[features].values, train['OutcomeType'].values
X_test, Y_test = test[features].values, test['OutcomeType'].values
```
This small following technical step is just a way to to adapt the form of Y_test to be able to use it as a parameter for the log_loss function I introduced earlier (similar to the $$y_{ij}$$)

```python
Y_test_bis = (Y_test[:, np.newaxis] == np.arange(5)[np.newaxis, :]).astype(np.int)
```

## Building some models, and using GridSearchCV:

I'm going to perform below a Grid Search to determine the best values of the hyperparameters for the upcoming model.

```python
# the following dictionary and we will use the GridSearchCV (crossvalidation) to find the best
# logloss value.
max_depth_guesses = (5, 10, 15, 20, 30, 40, 50)
param_grid = {
    'max_depth': max_depth_guesses
}
```

I'm going to first build a basic decision tree:

```python
decision_tree = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid)
decision_tree.fit(X_train, Y_train)
decision_tree_logloss = log_loss(Y_test_bis, decision_tree.predict_proba(X_test))
print('Log-loss of decision tree classifier: {0}'.format(decision_tree_logloss))
```

A random forest:

```python
decision_tree = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid)
decision_tree.fit(X_train, Y_train)
decision_tree_logloss = log_loss(Y_test_bis, decision_tree.predict_proba(X_test))
print('Log-loss of decision tree classifier: {0}'.format(decision_tree_logloss))
```

Finally, here's an implementation of gradient boosting.

```python
grad_boosting = GridSearchCV(GradientBoostingClassifier(random_state=0, n_estimators=100), 
                             param_grid=param_grid, verbose=True)
grad_boosting.fit(X_train, Y_train)
grad_boosting_logloss = log_loss(Y_test_bis, grad_boosting.predict_proba(X_test))
print('Log-loss of gradient boosting classifier: {0}'.format(grad_boosting_logloss))
```

Results:
```python
Log-loss of decision tree classifier: 0.9210953350935193
Log-loss of random forest classifier: 0.8467709000339786
Log-loss of gradient boosting classifier: 0.8334824132643417
```

It seems that the best model (among those three) is gradient boosting, as it achieves the lowest loss. 
But ! The big downside is that it took... 23 minutes to train ! While the random forest only took 25.4s to finish.

What's more, is that when I look at how these classifiers perform on subsets of the data (on cats, and dogs):


![alt]({{ site.url }}{{ site.baseurl }}/figures/algo8.PNG)

I get that the classifier performs much better on cats. Maybe I haven't extracted enough features for the dogs ?
Or maybe more data has to reveal some more patterns for the dogs. 
Ensemble learning methods could be tried, maybe by training a model exclusively for the dogs. But of course, the feature selection and extraction steps above can be discussed more in depth.


## The Destiny of Gus and Skooter

And now, the answer we've all been waiting for. What's next in store, for Gus and Skooter?
By applying the gradient boosting classifier trained above to the new data, we can compute the probabilities of each outcome for Gus and Skooter.

![alt]({{ site.url }}{{ site.baseurl }}/figures/gus1.PNG)


![alt]({{ site.url }}{{ site.baseurl }}/figures/gus2.PNG)


Looks like Gus is most likely to be transferred to another shelter/facility, while Skooter has a good chance at getting adopted ! Or even returned to their original owner. Unfortunately, not all animals are so lucky. That is the case of Summer, the first dog in the above table, most likely to be euthanized. 

But don't be sad: our prediction model - as we saw - isn't very accurate on dogs. That can be a good or a bad thing (for Gus and Skooter). However, while more data could be gathered and the procedure could be made better, there's one sure procedure we can implement to guarantee a beautiful outcome for every animal (with a 100% accuracy!): Why don't you stop by at [Animals Lebanon's website](http://www.animalslebanon.org/) (or any other shelter), and adopt one of those cute little munchkins ?

![alt]({{ site.url }}{{ site.baseurl }}/figures/animals.PNG)

This is how we could easily best the Gradient Boosting algorithm - despite it being one of the most efficient ones available to this day.