---
title: "NLP for customer analysis: turning words into numbers"
date: 2020-04-06
tags: [NLP, natural language processing, sentiment analysis, topic labelling, tidyverse, latent Dirichlet allocation, LDA, R]
header:
 image: 
excerpt: "Analyzing customer's comments to improve customer service a mobile company"
mathjax: "true"
---


![alt]({{ site.url }}{{ site.baseurl }}/stc/logostc.jpg)

[STC (Saudi Telecom Company)](https://en.wikipedia.org/wiki/Saudi_Telecom_Company) is one of the  most prominent digital companies in the Middle East with millions of subscribers to their services. 
But how does STC keep so many subscribers happy ? 

There's no doubt that they leverage on the feedback of their users and thankfully, they receive plenty of that: every day, STC's customer support receives hundreds of questions and requests. Hundreds of people flock everyday to their social media pages to voice their concerns or ask for technical assistance. The volume of this valuable feedback can easily become overwhelming even with an amazing and reactive customer support service.


![alt]({{ site.url }}{{ site.baseurl }}/stc/wonder.png)

Is it possible to use posts from Instagram, Facebook or an online chatting tool to draw useful quantitative insights ? Can words be transformed into numbers ? 


How can we convert a large bucket of words into quantifiable answers to questions like:

* How are the customers feeling in general ? Are there specific subsets of the customers who are more satisfied than others ?
* What are their main concerns ? Are there some technical problems specific to an area/location ?  Do some technical problems arise on specific days of the year ?
* Do some technical problems occur together ?
* Are employees responding promptly or positively to customers ?


I've had the opportunity to work on a project for STC at the beginning of 2020 in which I attempted to answer questions like the above using thousands of social media comments, as well as customer support center data. 
For obvious privacy concerns, I will not be using actual data from STC in this article. 

Instead, I am going to present the general methodology using mock data from one of their competitors: CrabMobile. 


* This is
{:toc}

### CrabMobile: An instagram introduction

CrabMobile is a telecom company based in Dubai and Bahrain.
They've just launched a new service and are eager to see how customers are reacting. Let's take a look at one of their instagram posts:

![alt]({{ site.url }}{{ site.baseurl }}/stc/insta.PNG)


Some users (@LSeb, @Lul) seem to have left positive comments while others (@Founa) don't seem too fond of the offering. Some ambiguous comments seem to be written in foreign languages (@Poot), while other users have written multiple comments by mistake (@Lul). 


### Turning words into numbers using sentiment dictionaries

One basic way we could turn words into numbers is by applying a sentiment lexicon. A sentiment lexicon is simply a way to assign a score (or a label) to a word, based on its connotation (most positive +5 to most negative -5). This is a snippet of the 'Afinn' lexicon:


```r
#> # A tibble: 2,477 x 2
#>    word       value
#>    <chr>      <dbl>
#>  1 abandon       -2
#>  2 abandoned     -2
#>  3 abandons      -2
#>  4 abducted      -2
#>  5 abduction     -2
#>  6 abductions    -2
#>  7 abhor         -3
#>  8 abhorred      -3
#>  9 abhorrent     -3
#> 10 abhors        -3
#> # … with 2,467 more rows
 
```

There are of course, other types of Lexicons like 'Loughran' and 'Bing'. The Loughran assigns to each word, one of the following labels:  "negative", "positive", "litigious", "uncertainty", "constraining", or "superfluous". 
How can we apply this to the instagram post we have seen above ?


### CrabMobile's dataset and sentiment analysis

CrabMobile's data science team performed some web scraping across platforms (Instagram, Facebook...) and provided me with an initial dataset.  Here are some of its categories. 

![alt]({{ site.url }}{{ site.baseurl }}/stc/categories.PNG)

A brief description of the most important columns:

- Medium: platform of the message/post (instagram, facebook...)
- Content:  contents of the post/message
- Direction: if the message was written by a customer it is ‘Inbound’ and if it is written by a staff member, ‘Outbound’.

I also had access to other information such as a 'Location' column as well as some details about the customers like 'Occupation'. 

#### Data Preprocessing and methodology

As we have seen in the instagram post, real world data is not as clean as a lexicon. Here are some of the technical difficulties that I came across when attempting to use lexicons:

1. Foreign words (in this case in Arabic): I need to address the fact that the ‘Content’ column comes in both languages Arabic and English (and sometimes a mix of the two). A possible way to do this is to use some translation API (however, these can be costly). Another way to do this would be to research available Arabic Sentiment Lexicons and to treat arabic words separately. 
2. Abbreviations and common words: Stopwords can be used to eliminate some of the most common words. We could also create a custom list of contextual common words.
3. Spam and multiple posts


As a first step, I'm going to unnest the words available in the ‘Content’ column and filter them using the DetectLanguage function. I have used the dplyr package (which provides us with English sentiment lexicons) as well as cld3 for language detection. 

Tidyverse and Tidytext are very resourceful libraries for sentiment analysis; the package ‘stopwords’ does include Arabic stop words as well !

#### Sentiment by platform

As a first step, I'm going to work with the English text only. 
I started by grouping the *inbound* posts  by 'platform' (instagram, facebook...) and then I applied some dictionaries to the inbound . Finally, I computed the sum of all word scores per platform. 

Here's a sample of my code to group by categories and apply lexicons on R:
```r
text1 <- df %>%
  unnest_tokens(word, Content) %>%
  filter(Language =='en')  %>%
  filter(Direction== 'inbound') %>%
  inner_join(get_sentiments("bing"))  %>%
  count(Medium, sentiment) %>%
  spread(sentiment, n) %>%
  mutate(overall_sentiment = positive - negative)
```

A small plot: 

```r
ggplot(
  text1,
  aes(x = Medium, y = overall_sentiment, fill = as.factor(Medium))
) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
	title = "Overall Sentiment by Medium",
	subtitle = "Which platform attracts positive/negative sentiment among customers? ",
	x = "Medium",
	y = "Overall Sentiment"
  )
```

Here's what I found:

 
![alt]({{ site.url }}{{ site.baseurl }}/stc/inboundscore.jpg)

This plot shows that customers used least positive words on instagram and twitter.
Let's analyze the operators' posts now (the *outbound* content)

![alt]({{ site.url }}{{ site.baseurl }}/stc/outboundscore.jpg)

We can see that employees do not give out very positive sentiment on Facebook ! Therefore, I'd like to suggest to CrabMobile's customer service to work a bit more on their Facebook posts !


####


### Topic Modelling using Latent Dirichlet Allocation


